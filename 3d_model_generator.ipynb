{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install streamlit -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8axQcurwDoL",
        "outputId": "79f9fb6d-a4bb-43f6-c640-782849944a63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/openai/shap-e.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTB7-nRswN3E",
        "outputId": "cbb4115c-d63e-4559-9f24-34ce074d0cda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'shap-e'...\n",
            "remote: Enumerating objects: 336, done.\u001b[K\n",
            "remote: Counting objects: 100% (55/55), done.\u001b[K\n",
            "remote: Compressing objects: 100% (42/42), done.\u001b[K\n",
            "remote: Total 336 (delta 35), reused 13 (delta 13), pack-reused 281 (from 2)\u001b[K\n",
            "Receiving objects: 100% (336/336), 11.72 MiB | 8.96 MiB/s, done.\n",
            "Resolving deltas: 100% (43/43), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd shap-e"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F22Ir74uwPbw",
        "outputId": "6bcd8271-f2d5-4518-bf01-9c1dea28d4c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/shap-e\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -e ."
      ],
      "metadata": {
        "id": "znC3rMWEwP_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D67XTp_GvvIO",
        "outputId": "e8078fb5-4148-482d-ad2a-75b120de6781"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "import torch\n",
        "from shap_e.diffusion.sample import sample_latents\n",
        "from shap_e.diffusion.gaussian_diffusion import diffusion_from_config\n",
        "from shap_e.models.download import load_model, load_config\n",
        "from shap_e.util.notebooks import create_pan_cameras, decode_latent_images, gif_widget\n",
        "from shap_e.util.image_util import load_image\n",
        "from IPython.display import display\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from shap_e.util.notebooks import decode_latent_mesh\n",
        "import io\n",
        "import time\n",
        "import tempfile\n",
        "\n",
        "st.title(\"create 3d model\")\n",
        "option = st.selectbox(\n",
        "    \"select the model\",\n",
        "    (\"image-to-3d\", \"text-to-3d\")\n",
        ")\n",
        "\n",
        "# Create a placeholder\n",
        "placeholder = st.empty()\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "if option == \"text-to-3d\":\n",
        "    prompt = st.text_input(\"Enter the text\")\n",
        "    if st.button(\"Create 3D Model\"):\n",
        "        # create 3d model using text\n",
        "        import os\n",
        "        import shutil\n",
        "\n",
        "        # This is the default cache directory for CLIP models\n",
        "        clip_cache_dir = os.path.expanduser(\"~/.cache/clip\")\n",
        "\n",
        "        # Remove the entire CLIP cache directory\n",
        "        if os.path.exists(clip_cache_dir):\n",
        "            shutil.rmtree(clip_cache_dir)\n",
        "        placeholder.success(\"loading model.....\")\n",
        "        # Re-run the model loading code\n",
        "        xm = load_model('transmitter', device=device)\n",
        "        text_model = load_model('text300M', device=device)  # This will now download a fresh copy\n",
        "        diffusion = diffusion_from_config(load_config('diffusion'))\n",
        "        time.sleep(5)\n",
        "        placeholder.empty()\n",
        "        placeholder.success(\"generating 3d models...\")\n",
        "        # generate 3d model\n",
        "        batch_size = 4\n",
        "        guidance_scale = 50.0\n",
        "        latents = sample_latents(\n",
        "            batch_size=batch_size,\n",
        "            model=text_model,\n",
        "            diffusion=diffusion,\n",
        "            guidance_scale=guidance_scale,\n",
        "            model_kwargs=dict(texts=[prompt] * batch_size),\n",
        "            progress=True,\n",
        "            clip_denoised=True,\n",
        "            use_fp16=True,\n",
        "            use_karras=True,\n",
        "            karras_steps=64,\n",
        "            sigma_min=1e-3,\n",
        "            sigma_max=160,\n",
        "            s_churn=0,\n",
        "        )\n",
        "\n",
        "        render_mode = 'nerf' # you can change this to 'stf'\n",
        "        size = 64 # this is the size of the renders; higher values take longer to render.\n",
        "\n",
        "        cameras = create_pan_cameras(size, device)\n",
        "        def show_gif_in_streamlit(images, duration=100, target_size=(64, 64)):\n",
        "          # images: list of PIL.Image\n",
        "          if not images:\n",
        "              st.warning(\"No images to display.\")\n",
        "              return\n",
        "\n",
        "          # Resize each image\n",
        "          resized_images = [img.resize(target_size) for img in images]\n",
        "\n",
        "          # Save images as GIF in memory\n",
        "          gif_buffer = io.BytesIO()\n",
        "          resized_images[0].save(\n",
        "              gif_buffer,\n",
        "              format='GIF',\n",
        "              save_all=True,\n",
        "              append_images=resized_images[1:],\n",
        "              duration=duration,\n",
        "              loop=0\n",
        "          )\n",
        "          gif_buffer.seek(0)\n",
        "\n",
        "          # Display resized GIF\n",
        "          st.image(gif_buffer, caption=\"3D Render (Animated)\")\n",
        "\n",
        "        for i, latent in enumerate(latents):\n",
        "            images = decode_latent_images(xm, latent, cameras, rendering_mode=render_mode)\n",
        "            time.sleep(5)\n",
        "            placeholder.empty()\n",
        "            #Visualization\n",
        "            placeholder.success(f\"{prompt} 3D model\")\n",
        "            show_gif_in_streamlit(images)\n",
        "            #mesh obj\n",
        "            t = decode_latent_mesh(xm, latent).tri_mesh()\n",
        "            obj_text = io.StringIO()\n",
        "            t.write_obj(obj_text)\n",
        "            obj_bytes = io.BytesIO(obj_text.getvalue().encode(\"utf-8\"))  # Convert text to bytes\n",
        "            obj_bytes.seek(0)\n",
        "            st.download_button(\n",
        "              label=f\"Download {prompt} Mesh {i} (.obj)\",\n",
        "              data=obj_bytes.getvalue(),\n",
        "              file_name=f\"{prompt}_{i}.obj\",\n",
        "              mime=\"text/plain\"\n",
        "          )\n",
        "else:\n",
        "  uploaded_image = st.file_uploader(\"Upload an image\", type=[\"png\", \"jpg\", \"jpeg\"])\n",
        "  if st.button(\"Create 3D Model\"):\n",
        "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".png\") as tmp:\n",
        "        tmp.write(uploaded_image.read())\n",
        "        tmp_path = tmp.name\n",
        "    # create 3d model using image\n",
        "\n",
        "    import os\n",
        "    import shutil\n",
        "\n",
        "    # This is the default cache directory for CLIP models\n",
        "    clip_cache_dir = os.path.expanduser(\"~/.cache/clip\")\n",
        "\n",
        "    # Remove the entire CLIP cache directory\n",
        "    if os.path.exists(clip_cache_dir):\n",
        "        shutil.rmtree(clip_cache_dir)\n",
        "    placeholder.success(\"loading model.....\")\n",
        "    # Re-run the model loading code\n",
        "    xm = load_model('transmitter', device=device)\n",
        "    image_model = load_model('image300M', device=device)\n",
        "    diffusion = diffusion_from_config(load_config('diffusion'))\n",
        "    time.sleep(5)\n",
        "    placeholder.empty()\n",
        "    placeholder.success(\"generating 3d models...\")\n",
        "    batch_size = 4\n",
        "    guidance_scale = 3.0\n",
        "\n",
        "    # To get the best result, you should remove the background and show only the object of interest to the model.\n",
        "    image = load_image(tmp_path)\n",
        "\n",
        "    latents = sample_latents(\n",
        "        batch_size=batch_size,\n",
        "        model=image_model,\n",
        "        diffusion=diffusion,\n",
        "        guidance_scale=guidance_scale,\n",
        "        model_kwargs=dict(images=[image] * batch_size),\n",
        "        progress=True,\n",
        "        clip_denoised=True,\n",
        "        use_fp16=True,\n",
        "        use_karras=True,\n",
        "        karras_steps=64,\n",
        "        sigma_min=1e-3,\n",
        "        sigma_max=160,\n",
        "        s_churn=0,\n",
        "    )\n",
        "    render_mode = 'nerf' # you can change this to 'stf'\n",
        "    size = 64 # this is the size of the renders; higher values take longer to render.\n",
        "\n",
        "    cameras = create_pan_cameras(size, device)\n",
        "    def show_gif_in_streamlit(images, duration=100, target_size=(64,64)):\n",
        "    # images: list of PIL.Image\n",
        "      if not images:\n",
        "          st.warning(\"No images to display.\")\n",
        "          return\n",
        "\n",
        "      # Resize each image\n",
        "      resized_images = [img.resize(target_size) for img in images]\n",
        "\n",
        "      # Save images as GIF in memory\n",
        "      gif_buffer = io.BytesIO()\n",
        "      resized_images[0].save(\n",
        "          gif_buffer,\n",
        "          format='GIF',\n",
        "          save_all=True,\n",
        "          append_images=resized_images[1:],\n",
        "          duration=duration,\n",
        "          loop=0\n",
        "      )\n",
        "      gif_buffer.seek(0)\n",
        "\n",
        "      # Display resized GIF\n",
        "      st.image(gif_buffer, caption=\"3D Render (Animated)\")\n",
        "\n",
        "    for i, latent in enumerate(latents):\n",
        "        images = decode_latent_images(xm, latent, cameras, rendering_mode=render_mode)\n",
        "        time.sleep(5)\n",
        "        placeholder.empty()\n",
        "        #Visualization\n",
        "        placeholder.success(f\"3D model\")\n",
        "        show_gif_in_streamlit(images)\n",
        "        #mesh obj\n",
        "        t = decode_latent_mesh(xm, latent).tri_mesh()\n",
        "        obj_text = io.StringIO()\n",
        "        t.write_obj(obj_text)\n",
        "        obj_bytes = io.BytesIO(obj_text.getvalue().encode(\"utf-8\"))  # Convert text to bytes\n",
        "        obj_bytes.seek(0)\n",
        "        st.download_button(\n",
        "          label=f\"Download  Mesh {i} (.obj)\",\n",
        "          data=obj_bytes.getvalue(),\n",
        "          file_name=f\"mesh_{i}.obj\",\n",
        "          mime=\"text/plain\"\n",
        "      )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q -O - ipv4.icanhazip.com"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "to7L02jkwEvs",
        "outputId": "e99d282b-4150-421a-e55a-50fdb4e69fa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.143.130.224\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! streamlit run app.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbLxbzCXwHv5",
        "outputId": "2377ee2c-3992-4946-a62a-eb3b87026f55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0K⠙\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.143.130.224:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0Kyour url is: https://crazy-mangos-glow.loca.lt\n",
            "2025-05-04 12:06:52.254 Examining the path of torch.classes raised:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/web/bootstrap.py\", line 347, in run\n",
            "    if asyncio.get_running_loop().is_running():\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: no running event loop\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 217, in get_module_paths\n",
            "    potential_paths = extract_paths(module)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 210, in <lambda>\n",
            "    lambda m: list(m.__path__._path),\n",
            "                   ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_classes.py\", line 13, in __getattr__\n",
            "    proxy = torch._C._get_custom_class_python_wrapper(self.name, attr)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: Tried to instantiate class '__path__._path', but it does not exist! Ensure that it is registered via torch::class_\n",
            "/content/shap-e/shap_e/models/nn/checkpoint.py:31: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @custom_fwd\n",
            "/content/shap-e/shap_e/models/nn/checkpoint.py:43: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  @custom_bwd\n",
            "/content/shap-e/shap_e/models/nn/checkpoint.py:61: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "  @custom_fwd\n",
            "/content/shap-e/shap_e/models/nn/checkpoint.py:86: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "  @custom_bwd\n",
            "100% 64/64 [01:57<00:00,  1.84s/it]\n",
            "/content/shap-e/shap_e/models/stf/renderer.py:286: UserWarning: exception rendering with PyTorch3D: No module named 'pytorch3d'\n",
            "  warnings.warn(f\"exception rendering with PyTorch3D: {exc}\")\n",
            "/content/shap-e/shap_e/models/stf/renderer.py:287: UserWarning: falling back on native PyTorch renderer, which does not support full gradients\n",
            "  warnings.warn(\n",
            "2025-05-04 12:10:41.014 Examining the path of torch.classes raised:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/web/bootstrap.py\", line 347, in run\n",
            "    if asyncio.get_running_loop().is_running():\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: no running event loop\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 217, in get_module_paths\n",
            "    potential_paths = extract_paths(module)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 210, in <lambda>\n",
            "    lambda m: list(m.__path__._path),\n",
            "                   ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_classes.py\", line 13, in __getattr__\n",
            "    proxy = torch._C._get_custom_class_python_wrapper(self.name, attr)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: Tried to instantiate class '__path__._path', but it does not exist! Ensure that it is registered via torch::class_\n",
            "100% 64/64 [01:57<00:00,  1.84s/it]\n",
            "2025-05-04 12:24:33.071 Examining the path of torch.classes raised:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/web/bootstrap.py\", line 347, in run\n",
            "    if asyncio.get_running_loop().is_running():\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: no running event loop\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 217, in get_module_paths\n",
            "    potential_paths = extract_paths(module)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 210, in <lambda>\n",
            "    lambda m: list(m.__path__._path),\n",
            "                   ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_classes.py\", line 13, in __getattr__\n",
            "    proxy = torch._C._get_custom_class_python_wrapper(self.name, attr)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: Tried to instantiate class '__path__._path', but it does not exist! Ensure that it is registered via torch::class_\n",
            "100% 64/64 [01:58<00:00,  1.85s/it]\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7kfvQVbrw4pa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}